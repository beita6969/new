# Qwen2.5-7B Workflow Optimizer - Full Training Configuration
# Fixed domain configuration + Increased to 1000 steps for complete training
#
# This config trains Qwen2.5-7B to replace API calls in AFlow workflow optimization
# - Keeps all AFlow operators and prompts intact
# - Uses ROLL GRPO algorithm for RL training
# - Qwen model controls: prompt optimization, operator selection, edge relationships
# - FIXED: domain_interleave_probs now matches rewards configuration (llm_judge)
# - INCREASED: max_steps from 50 to 1000 for full training

# Hydra Configuration (from official example)
hydra:
  run:
    dir: .
  output_subdir: null

# Global Settings (from official example)
exp_name: "qwen2.5-7B-workflow-optimizer-full-training"
seed: 42
logging_dir: ./output/logs
output_dir: ./output

# System Environment
system_envs:
  USE_MODELSCOPE: '1'

# Checkpoint Configuration
checkpoint_config:
  type: file_system
  output_dir: data/checkpoints/${exp_name}

# Tracking - Using WandB
track_with: wandb
tracker_kwargs:
  project: roll-workflow-optimizer
  name: ${exp_name}
  notes: "Training Qwen2.5-7B for AFlow workflow optimization using GRPO"
  tags:
    - workflow-optimization
    - qwen2.5-7b
    - grpo
    - rlvr

# GPU Configuration (Single GPU)
num_gpus_per_node: 1

# Training Configuration (FULL TRAINING: 1000 steps)
max_steps: 1000  # INCREASED from 50 for full training
save_steps: 100
logging_steps: 1
eval_steps: 50  # Evaluate every 50 steps
resume_from_checkpoint: false

# Rollout Configuration (from official example)
rollout_batch_size: 8  # Reduced for single GPU
prompt_length: 3072
response_length: 3072

# GRPO Configuration (from official example)
num_return_sequences_in_group: 4  # Reduced from 8 for memory
ppo_epochs: 1
adv_estimator: "grpo"  # Use GRPO instead of reinforce

# Clipping (from official example)
value_clip: 0.5
reward_clip: 10
advantage_clip: 2.0
dual_clip_loss: true

# Normalization (GRPO specific)
norm_mean_type: "group"  # GRPO uses group normalization
norm_std_type: "group"

# Data Masking (from official example)
max_len_mask: true
difficulty_mask: false  # Disabled for our data
difficulty_low_threshold: 0.1
difficulty_high_threshold: 0.95
error_max_len_clip: false

# Data Weighting (from official example)
difficulty_loss_weight: false
length_loss_weight: false

# Reward (from official example)
add_token_level_kl: false

# Advantage (from official example)
whiten_advantages: true

# Model Configuration
pretrain: Qwen/Qwen2.5-7B-Instruct
reward_pretrain: Qwen/Qwen2.5-7B-Instruct

# LoRA Configuration (for single GPU memory efficiency)
lora_target: o_proj,q_proj,k_proj,v_proj
lora_rank: 32
lora_alpha: 32

# Validation Configuration (from official example)
validation:
  data_args:
    template: qwen2_5
    file_name:
      - data/rl_training_data_full/val_data.jsonl
    preprocessing_num_workers: ~  # None for single GPU
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.6
    top_k: 50
    num_beams: 1
    temperature: 0.6
    num_return_sequences: 1

# Actor Training Configuration (Single GPU + LoRA)
actor_train:
  model_args:
    model_name_or_path: ${pretrain}
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
    disable_gradient_checkpointing: false  # Enable for memory saving
    dtype: bf16
    model_type: ~

  training_args:
    learning_rate: 1.0e-6
    weight_decay: 0
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 32
    warmup_steps: 20
    num_train_epochs: 50
    max_grad_norm: 1.0
    logging_steps: ${logging_steps}
    save_steps: ${save_steps}

  data_args:
    template: qwen2_5  # CRITICAL: Use qwen2_5 template
    file_name:
      - data/rl_training_data_full/train_data.jsonl  # 477 AFlow workflow samples
    domain_interleave_probs:
      llm_judge: 1.0  # FIXED: Must match rewards configuration key
    dataset_dir: data
    messages: messages  # CRITICAL: Specify messages field explicitly
    interleave_probs: "1.0"
    preprocessing_num_workers: ~  # CRITICAL: None to avoid serialization issues

  strategy_args:
    strategy_name: deepspeed_train  # Required for LoRA
    strategy_config:
      zero_optimization:
        stage: 2  # Stage 2 for single GPU
        overlap_comm: true
        contiguous_gradients: true
        reduce_bucket_size: 5.0e8
        allgather_bucket_size: 5.0e8
        cpu_offload: false  # 80GB should be sufficient
      gradient_accumulation_steps: ${actor_train.training_args.gradient_accumulation_steps}
      train_micro_batch_size_per_gpu: ${actor_train.training_args.per_device_train_batch_size}
      gradient_clipping: ${actor_train.training_args.max_grad_norm}
      bf16:
        enabled: true
      zero_allow_untested_optimizer: true

  device_mapping: list(range(0,1))  # Single GPU
  infer_batch_size: 1

# Actor Inference Configuration (Single GPU)
actor_infer:
  model_args:
    model_name_or_path: ${pretrain}
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~

  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.99
    num_return_sequences: ${num_return_sequences_in_group}

  data_args:
    template: qwen2_5

  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.7  # Leave room for training
      block_size: 16
      max_model_len: 6144  # prompt + response
      trust_remote_code: true
      tensor_parallel_size: 1

  device_mapping: list(range(0,1))
  infer_batch_size: 1

# Reference Model Configuration (Single GPU)
reference:
  model_args:
    model_name_or_path: ${pretrain}
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~

  data_args:
    template: qwen2_5

  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1

  device_mapping: list(range(0,1))
  infer_batch_size: 2

# Reward Configuration (Using pre-computed performance_gain from data)
rewards:
  llm_judge:  # CRITICAL: Must match domain_interleave_probs key
    worker_cls: roll.pipeline.rlvr.rewards.performance_gain_reward_worker.PerformanceGainRewardWorker
    reward_type: continuous
    reward_scale: 10.0  # Scale performance_gain to match GRPO reward scale
    clip_rewards: false  # Don't clip since gains are already normalized
    world_size: 1  # Single GPU
    infer_batch_size: 4  # Batch size for reward computation
    tag_included:
      - GSM8K
      - MATH
      - HumanEval
      - MBPP
      - HotpotQA
      - DROP
